-----exactitud de prediccion---------------
acuracy
import pandas as pd #importamos pandas
import sklearn #biblioteca de aprendizaje autom치tico
import matplotlib.pyplot as plt #Librer칤a especializada en la creaci칩n de gr치ficos
from sklearn.decomposition import PCA #importamos algorimo PCA
from sklearn.decomposition import IncrementalPCA #importamos algorimo PCA
from sklearn.linear_model import LogisticRegression #clasificaci칩n y an치lisis predictivo 
from sklearn.preprocessing import StandardScaler #Normalizar los datos
from sklearn.model_selection import train_test_split #permite hacer una divisi칩n de un conjunto de datos en dos bloques de entrenamiento y prueba de un modelo

if __name__ == '__main__':
    dt_heart=pd.read_csv('./data/heart.csv')
    
    #print(dt_heart.head(15)) #imprimimos los 5 primeros datos
    
    dt_features=dt_heart.drop(['target'],axis=1) #las featurus sin el target
    dt_target = dt_heart['target'] #obtenemos el target

    dt_features = StandardScaler().fit_transform(dt_features) #Normalizamnos los datos
   
    X_train,X_test,y_train,y_test =train_test_split(dt_features,dt_target,test_size=0.30,random_state=42)
    print(X_train.shape) #consultar la forma de la tabla con pandas
    print(y_train.shape)
    '''EL n칰mero de componentes es opcional, ya que por defecto si no le pasamos el n칰mero de componentes lo asignar치 de esta forma:
    a: n_components = min(n_muestras, n_features)'''
    pca=PCA(n_components=4)
    # Esto para que nuestro PCA se ajuste a los datos de entrenamiento que tenemos como tal
    pca.fit(X_train)
     #Como haremos una comparaci칩n con incremental PCA, haremos lo mismo para el IPCA.
    '''EL par치metro batch se usa para crear peque침os bloques, de esta forma podemos ir entrenandolos
    poco a poco y combinarlos en el resultado final'''
    ipca=IncrementalPCA(n_components=3,batch_size=10) #tama침o de bloques, no manda a entrear todos los datos
    #Esto para que nuestro PCA se ajuste a los datos de entrenamiento que tenemos como tal
    ipca.fit(X_train)
    ''' Aqu칤 graficamos los n칰meros de 0 hasta la longitud de los componentes que me sugiri칩 el PCA o que
    me gener칩 autom치ticamente el pca en el eje x, contra en el eje y, el valor de la importancia
    en cada uno de estos componentes, as칤 podremos identificar cu치les son realmente importantes
    para nuestro modelo '''
    plt.plot(range(len(pca.explained_variance_)),pca.explained_variance_ratio_) #gneera  desde 0 hasta los componentes
    #plt.show()
    #Ahora vamos a configurar nuestra regresi칩n log칤stica
    logistic=LogisticRegression(solver='lbfgs')
     # Configuramos los datos de entrenamiento
    dt_train = pca.transform(X_train)#conjunto de entrenamiento
    dt_test = pca.transform(X_test)#conjunto de prueba
     # Mandamos los data frames la la regresi칩n log칤stica
    logistic.fit(dt_train, y_train) #mandasmos a regresion logistica los dos datasets
    #Calculamos nuestra exactitud de nuestra predicci칩n
    print("SCORE PCA: ", logistic.score(dt_test, y_test))
    
    #Configuramos los datos de entrenamiento
    dt_train = ipca.transform(X_train)
    dt_test = ipca.transform(X_test)
    # Mandamos los data frames la la regresi칩n log칤stica
    logistic.fit(dt_train, y_train)
    #Calculamos nuestra exactitud de nuestra predicci칩n
    print("SCORE IPCA: ", logistic.score(dt_test, y_test))

-------Comparaci칩n entre PCA e IPCA
Es interesante ver c칩mo cambia la exactitud del modelo en funci칩n del n칰mero de componentes que se elijan tanto para el IPCA como para el PCA.

import pandas as pd #importamos pandas
import sklearn #biblioteca de aprendizaje autom치tico
import matplotlib.pyplot as plt #Librer칤a especializada en la creaci칩n de gr치ficos
from sklearn.decomposition import PCA #importamos algorimo PCA
from sklearn.decomposition import IncrementalPCA #importamos algorimo PCA
from sklearn.linear_model import LogisticRegression #clasificaci칩n y an치lisis predictivo 
from sklearn.preprocessing import StandardScaler #Normalizar los datos
from sklearn.model_selection import train_test_split #permite hacer una divisi칩n de un conjunto de datos en dos bloques de entrenamiento y prueba de un modelo

if __name__ == '__main__':
    dt_heart=pd.read_csv('./data/heart.csv')
    
    #print(dt_heart.head(15)) #imprimimos los 5 primeros datos
    
    dt_features=dt_heart.drop(['target'],axis=1) #las featurus sin el target
    dt_target = dt_heart['target'] #obtenemos el target

    dt_features = StandardScaler().fit_transform(dt_features) #Normalizamnos los datos
   
    X_train,X_test,y_train,y_test =train_test_split(dt_features,dt_target,test_size=0.30,random_state=42)
    #print(X_train.shape) #consultar la forma de la tabla con pandas
    #print(y_train.shape)
    '''EL n칰mero de componentes es opcional, ya que por defecto si no le pasamos el n칰mero de componentes lo asignar치 de esta forma:
    a: n_components = min(n_muestras, n_features)'''
    pca=PCA(n_components=3)
    # Esto para que nuestro PCA se ajuste a los datos de entrenamiento que tenemos como tal
    pca.fit(X_train)
     #Como haremos una comparaci칩n con incremental PCA, haremos lo mismo para el IPCA.
    '''EL par치metro batch se usa para crear peque침os bloques, de esta forma podemos ir entrenandolos
    poco a poco y combinarlos en el resultado final'''
    ipca=IncrementalPCA(n_components=3,batch_size=10) #tama침o de bloques, no manda a entrear todos los datos
    #Esto para que nuestro PCA se ajuste a los datos de entrenamiento que tenemos como tal
    ipca.fit(X_train)
    ''' Aqu칤 graficamos los n칰meros de 0 hasta la longitud de los componentes que me sugiri칩 el PCA o que
    me gener칩 autom치ticamente el pca en el eje x, contra en el eje y, el valor de la importancia
    en cada uno de estos componentes, as칤 podremos identificar cu치les son realmente importantes
    para nuestro modelo '''
    #plt.plot(range(len(pca.explained_variance_)),pca.explained_variance_ratio_) #gneera  desde 0 hasta los componentes
    #plt.show()
    #Ahora vamos a configurar nuestra regresi칩n log칤stica
    logistic = LogisticRegression(solver='lbfgs')

    pca_data = {'accuracy': [],
                'n_components': []}
    ipca_data = {'accuracy': [],
                'n_components': []}
    # PCA
    for n in range(2, 10):
        pca = PCA(n_components=n)
        pca.fit(X_train)
        df_train = pca.transform(X_train)
        df_test = pca.transform(X_test)
        logistic.fit(df_train, y_train)
        acccuracy = logistic.score(df_test, y_test)
        
        pca_data['accuracy'].append(acccuracy)
        pca_data['n_components'].append(n)
    
    # IPC
    for n in range(2, 10):
        ipca = IncrementalPCA(n_components=n, batch_size=10)
        ipca.fit(X_train)
        df_train = ipca.transform(X_train)
        df_test = ipca.transform(X_test)
        logistic.fit(df_train, y_train)
        acccuracy = logistic.score(df_test, y_test)
        
        ipca_data['accuracy'].append(acccuracy)
        ipca_data['n_components'].append(n)
    
    
    plt.plot(pca_data['n_components'], pca_data['accuracy'], label='PCA')
    plt.plot(ipca_data['n_components'], ipca_data['accuracy'], label='IPCA')
    plt.title('N Components vs Accuracy - PCA vs IPCA')
    plt.xlabel('Number of Components')
    plt.ylabel('Accuracy of Logistic-Regression')
    plt.legend()
    plt.show()
---------------------------------
Ahora que ya sabemos para el algoritmo de PCA, 쯤ue otras alternativas tenemos?

Bueno, una alternativa son los Kernels. Un Kernel es una funci칩n matem치tica que toma mediciones que se comportan de manera no lineal y las proyecta en un espacio dimensional m치s grande en donde sen linealmente separables.

Y, 쯘sto para que puede servir?

1.PNG
Sirve para casos en donde no son linealmente separables. El la primera imagen no es posible separarlos con una linea y en la imagen 2 si lo podemos hacer mediante Kernels. Lo que hace la funci칩n de Kernels es proyectar los puntos en otra dimensi칩n y as칤 volver los datos linealmente separables.

쯈ue tipo de funciones para Kernels nos podemos encontrar?

2.PNG
Ejemplos de funciones de Kernels en datasets aplicados a un clasificador:

3.PNG
-------------kernel--
El texto trata sobre el concepto de Kernel y su aplicaci칩n en la clasificaci칩n de datos complejos a trav칠s de funciones matem치ticas 칰tiles. Se menciona que el Kernel es una funci칩n matem치tica que nos ayuda a proyectar datos de una dimensi칩n determinada a dimensiones m치s altas, lo que permite clasificar datos que no son linealmente separables. Se mencionan algunas opciones de los Kernels m치s comunes, como los Kernels lineales y los Kernels polin칩micos. Se explica que la estrategia del Kernel resulta computacionalmente eficiente y puede llevar a resultados muy buenos en la clasificaci칩n de datos complejos. Se presenta un ejemplo de implementaci칩n de Kernel en Python para la clasificaci칩n de pacientes con problemas card칤acos.
......Kernel y kpca....
comparativa de los tres kernel
import pandas as pd #importamos pandas
import sklearn #biblioteca de aprendizaje autom치tico
import matplotlib.pyplot as plt #Librer칤a especializada en la creaci칩n de gr치ficos

from sklearn.decomposition import KernelPCA #importamos algorimo PCA
from sklearn.decomposition import IncrementalPCA #importamos algorimo PCA
from sklearn.linear_model import LogisticRegression #clasificaci칩n y an치lisis predictivo 
from sklearn.preprocessing import StandardScaler #Normalizar los datos
from sklearn.model_selection import train_test_split #permite hacer una divisi칩n de un conjunto de datos en dos bloques de entrenamiento y prueba de un modelo

if __name__ == "__main__":
 # Cargamos los datos del dataframe de pandas
 dt_heart=pd.read_csv('./data/heart.csv')
 # Imprimimos un encabezado con los primeros 5 registros
 print(dt_heart.head(5))
 # Guardamos nuestro dataset sin la columna de target
 dt_features = dt_heart.drop(['target'], axis=1)
 # Este ser치 nuestro dataset, pero sin la columna
 dt_target = dt_heart['target']
 # Normalizamos los datos
 dt_features = StandardScaler().fit_transform(dt_features)
  # Partimos el conjunto de entrenamiento y para a침adir replicabilidad usamos el random state
 X_train, X_test, y_train, y_test = train_test_split(dt_features, dt_target, test_size=0.3, random_state=42)
 
 kernel = ['linear','poly','rbf']
 #Aplicamos la funci칩n de kernel de tipo polinomial
 for k in kernel:
        kpca = KernelPCA(n_components=4, kernel = k)
        #kpca = KernelPCA(n_components=4, kernel='poly' )
        #Vamos a ajustar los datos
        kpca.fit(X_train)
        
        #Aplicamos el algoritmo a nuestros datos de prueba y de entrenamiento
        dt_train = kpca.transform(X_train)
        dt_test = kpca.transform(X_test)
        
        #Aplicamos la regresi칩n log칤stica un vez que reducimos su dimensionalidad
        logistic = LogisticRegression(solver='lbfgs')
        
        #Entrenamos los datos
        logistic.fit(dt_train, y_train)
        
        #Imprimimos los resultados
        print("SCORE KPCA " + k + " : ", logistic.score(dt_test, y_test))
 ------------------------------------
쯈u칠 es la regularizaci칩n y c칩mo aplicarla?
En esta clase se habla de la t칠cnica de regularizaci칩n, que es utilizada para reducir la complejidad del modelo y mejorar su generalizaci칩n en escenarios de ver feeding. Se introduce un poco de sesgo para reducir la varianza de los datos y se necesita un t칠rmino adicional, la p칠rdida, para aplicar la regularizaci칩n. Se explican los tres tipos de regularizaci칩n: L1, L2 y Elastic Net, que se utilizan para eliminar o penalizar las variables que son irrelevantes para el modelo. Tambi칠n se mencionan los conceptos de p칠rdida y optimizaci칩n para trabajar con modelos lineales como la regresi칩n lineal a trav칠s del m칠todo de los m칤nimos cuadrados.
---------regularizacion data felicidad--------------
linear,lasso,ridge
# Importamos las bibliotecas
import pandas as pd
import sklearn

# Importamos los modelos de sklearn 
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge

# Importamos las metricas de entrenamiento y el error medio cuadrado
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

if __name__ == "__main__":
    
    # Importamos el dataset del 2017 
    dataset = pd.read_csv('./data/felicidad.csv')
    # Mostramos el reporte estadistico
    print(dataset.describe())

    # Vamos a elegir los features que vamos a usar
    X = dataset[['gdp', 'family', 'lifexp', 'freedom' , 'corruption' , 'generosity', 'dystopia']]
    # Definimos nuestro objetivo, que sera nuestro data set, pero solo en la columna score 
    y = dataset[['score']]

    # Imprimimos los conjutos que creamos 
    # En nuestros features tendremos definidos 155 registros, uno por cada pais, 7 colunas 1 por cada pais 
    print(X.shape)
    # Y 155 para nuestra columna para nuestro target 
    print(y.shape)

    # Aqu칤 vamos a partir nuestro entrenaminto en training y test, no hay olvidar el orden
    # Con el test size elejimos nuestro porcetaje de datos para training 
    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25)

    # Aqu칤 definimos nuestros regresores uno por 1 y llamamos el fit o ajuste 
    modelLinear = LinearRegression().fit(X_train, y_train)
    # Vamos calcular la prediccion que nos bota con la funcion predict con la regresion lineal 
    # y le vamos a mandar el test 
    y_predict_linear = modelLinear.predict(X_test)

    # Configuramos alpha, que es valor labda y entre mas valor tenga alpha en lasso mas penalizacion 
    # vamos a tener y lo entrenamos con la funci칩n fit 
    modelLasso = Lasso(alpha=0.2).fit(X_train, y_train)
    # Hacemos una prediccion para ver si es mejor o peor de lo que teniamos en el modelo lineal sobre
    # exactamente los mismos datos que ten칤amos anteriormente 
    y_predict_lasso = modelLasso.predict(X_test)

    # Hacemos la misma predicci칩n, pero para nuestra regresion ridge 
    modelRidge = Ridge(alpha=1).fit(X_train, y_train)
    # Calculamos el valor predicho para nuestra regresi칩n ridge 
    y_predict_ridge = modelRidge.predict(X_test)

    # Calculamos la perdida para cada uno de los modelos que entrenamos, empezaremos con nuestro modelo 
    # lineal, con el error medio cuadratico y lo vamos a aplicar con los datos de prueba con la prediccion 
    # que hicimos 
    linear_loss = mean_squared_error(y_test, y_predict_linear)
    # Mostramos la perdida lineal con la variable que acabamos de calcular
    
    print( "Linear Loss. "+"%.10f" % float(linear_loss))
    # Mostramos nuestra perdida Lasso, con la variable lasso loss 
    lasso_loss = mean_squared_error(y_test, y_predict_lasso)
    print("Lasso Loss. "+"%.10f" % float( lasso_loss))

    # Mostramos nuestra perdida de Ridge con la variable lasso loss 
    ridge_loss = mean_squared_error(y_test, y_predict_ridge)
    print("Ridge loss: "+"%.10f" % float(ridge_loss))

    # Imprimimos las coficientes para ver como afecta a cada una de las regresiones 
    # La lines "="*32 lo unico que hara es repetirme si simbolo de igual 32 veces 
    print("="*32)
    print("Coeficientes linear: ")
    # Esta informacion la podemos encontrar en la variable coef_ 
    print(modelLinear.coef_)

    print("="*32)
    print("Coeficientes lasso: ")
    # Esta informacion la podemos encontrar en la variable coef_ 
    print(modelLasso.coef_)

    # Hacemos lo mismo con ridge 
    print("="*32)
    print("Coeficientes ridge:")
    print(modelRidge.coef_) 
    #Calculamos nuestra exactitud de nuestra predicci칩n lineal
    print("="*32)
    print("Score Lineal",modelLinear.score(X_test,y_test))
    #Calculamos nuestra exactitud de nuestra predicci칩n Lasso
    print("="*32)
    print("Score Lasso",modelLasso.score(X_test,y_test))
     #Calculamos nuestra exactitud de nuestra predicci칩n Ridge
    print("="*32)
    print("Score Ridge",modelRidge.score(X_test,y_test)) 
-------------ElasticNet---------------

El ElasticNet es una t칠cnica intermedia de regularizaci칩n que combina las dos t칠cnicas previas de regularizaci칩n, Lasso y Ridge, en una sola funci칩n. Al incluir un par치metro adicional, 洧띅, que var칤a entre 0 y 1, nos permite seleccionar cualquier combinaci칩n de L1 y L2. De esta manera, se superan las limitaciones de ambas t칠cnicas y se puede probar ambas al mismo tiempo sin perder informaci칩n.
ElasticNet puede ser una buena opci칩n si no se tiene suficiente experiencia o conocimiento matem치tico para elegir entre Lasso y Ridge, ya que ofrece una variedad de combinaciones lineales de ambas.
Para implementar ElasticNet en Python, se puede usar el m칩dulo linear_model de la biblioteca Scikit-learn. El primer paso es importar la clase ElasticNet, luego se inicializa un objeto ElasticNet y se entrena el modelo usando la funci칩n fit().
-----------------------Elastic net---------------
# Importamos las bibliotecas
import pandas as pd
import sklearn

# Importamos los modelos de sklearn 
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
# Importamos las metricas de entrenamiento y el error medio cuadrado
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

if __name__ == "__main__":
    
    # Importamos el dataset del 2017 
    dataset = pd.read_csv('./data/felicidad.csv')
    # Mostramos el reporte estadistico
    print(dataset.describe())

    # Vamos a elegir los features que vamos a usar
    X = dataset[['gdp', 'family', 'lifexp', 'freedom' , 'corruption' , 'generosity', 'dystopia']]
    # Definimos nuestro objetivo, que sera nuestro data set, pero solo en la columna score 
    y = dataset[['score']]

    # Imprimimos los conjutos que creamos 
    # En nuestros features tendremos definidos 155 registros, uno por cada pais, 7 colunas 1 por cada pais 
    print(X.shape)
    # Y 155 para nuestra columna para nuestro target 
    print(y.shape)

    # Aqu칤 vamos a partir nuestro entrenaminto en training y test, no hay olvidar el orden
    # Con el test size elejimos nuestro porcetaje de datos para training 
    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25)

    # Aqu칤 definimos nuestros regresores uno por 1 y llamamos el fit o ajuste 
    modelLinear = LinearRegression().fit(X_train, y_train)
    # Vamos calcular la prediccion que nos bota con la funcion predict con la regresion lineal 
    # y le vamos a mandar el test 
    y_predict_linear = modelLinear.predict(X_test)

    # Configuramos alpha, que es valor labda y entre mas valor tenga alpha en lasso mas penalizacion 
    # vamos a tener y lo entrenamos con la funci칩n fit 
    modelLasso = Lasso(alpha=0.2).fit(X_train, y_train)
    # Hacemos una prediccion para ver si es mejor o peor de lo que teniamos en el modelo lineal sobre
    # exactamente los mismos datos que ten칤amos anteriormente 
    y_predict_lasso = modelLasso.predict(X_test)

    # Hacemos la misma predicci칩n, pero para nuestra regresion ridge 
    modelRidge = Ridge(alpha=1).fit(X_train, y_train)
    # Calculamos el valor predicho para nuestra regresi칩n ridge 
    y_predict_ridge = modelRidge.predict(X_test)
    # Hacemos la misma predicci칩n, pero para nuestra regresion ElasticNet 
    modelElasticNet = ElasticNet(random_state=0).fit(X_train, y_train)
    # Calculamos el valor predicho para nuestra regresi칩n ElasticNet 
    y_pred_elastic = modelElasticNet.predict(X_test)
    # Calculamos la perdida para cada uno de los modelos que entrenamos, empezaremos con nuestro modelo 
    # lineal, con el error medio cuadratico y lo vamos a aplicar con los datos de prueba con la prediccion 
    # que hicimos 
    linear_loss = mean_squared_error(y_test, y_predict_linear)
    # Mostramos la perdida lineal con la variable que acabamos de calcular
    print( "Linear Loss. "+"%.10f" % float(linear_loss))

    # Mostramos nuestra perdida Lasso, con la variable lasso loss 
    lasso_loss = mean_squared_error(y_test, y_predict_lasso)
    print("Lasso Loss. "+"%.10f" % float( lasso_loss))

    # Mostramos nuestra perdida de Ridge con la variable Ridge loss 
    ridge_loss = mean_squared_error(y_test, y_predict_ridge)
    print("Ridge loss: "+"%.10f" % float(ridge_loss))

    # Mostramos nuestra perdida de ElasticNet con la variable Elastic loss
    elastic_loss = mean_squared_error(y_test, y_pred_elastic)
    print("ElasticNet Loss:  "+"%.10f" % float(elastic_loss))
    # Imprimimos las coficientes para ver como afecta a cada una de las regresiones 
    # La lines "="*32 lo unico que hara es repetirme si simbolo de igual 32 veces 
    print("="*32)
    print("Coeficientes linear: ")
    # Esta informacion la podemos encontrar en la variable coef_ 
    print(modelLinear.coef_)

    print("="*32)
    print("Coeficientes lasso: ")
    # Esta informacion la podemos encontrar en la variable coef_ 
    print(modelLasso.coef_)

    # Hacemos lo mismo con ridge 
    print("="*32)
    print("Coeficientes ridge:")
    print(modelRidge.coef_) 

     # Hacemos lo mismo con elastic 
    print("="*32)
    print("Coeficientes elastic net:")
    print(modelElasticNet.coef_) 

    #Calculamos nuestra exactitud de nuestra predicci칩n lineal
    print("="*32)
    print("Score Lineal",modelLinear.score(X_test,y_test))
    #Calculamos nuestra exactitud de nuestra predicci칩n Lasso
    print("="*32)
    print("Score Lasso",modelLasso.score(X_test,y_test))
     #Calculamos nuestra exactitud de nuestra predicci칩n Ridge
    print("="*32)
    print("Score Ridge",modelRidge.score(X_test,y_test))
     #Calculamos nuestra exactitud de nuestra predicci칩n Elastic Net
    print("="*32)
    print("Score Ridge",modelElasticNet.score(X_test,y_test))

--------------valores atipicos----------------
En este m칩dulo se habla de c칩mo lidiar con valores at칤picos en la preparaci칩n de datos para un modelo de aprendizaje autom치tico. Un valor at칤pico se define como un dato que no se comporta como el patr칩n general de los dem치s datos. Los valores at칤picos pueden ser problem치ticos ya que pueden sesgar los modelos y generar errores al hacer predicciones con datos del mundo real.
Existen dos m칠todos para identificar valores at칤picos: el m칠todo estad칤stico y matem치tico, y el m칠todo gr치fico. El m칠todo estad칤stico incluye el c치lculo del Z score y la t칠cnica de Klaus Terim. El m칠todo gr치fico utiliza gr치ficas espec칤ficas llamadas boxplots para visualizar la distribuci칩n de los datos.
Una vez que se identifican los valores at칤picos, se pueden utilizar varias t칠cnicas de preprocesamiento para tratarlos. Adem치s, hay modelos de clasificaci칩n y regresi칩n que autom치ticamente pueden lidiar con estos valores.
Es importante identificar y tratar los valores at칤picos en la preparaci칩n de datos para asegurar la precisi칩n de los modelos de aprendizaje autom치tico
-------------regresion robusta-------------------------
En esta clase se presenta un m칠todo para tratar con valores at칤picos en modelos de aprendizaje autom치tico. Aunque se pueden eliminar o transformar estos valores durante la etapa de preprocesamiento, a veces es necesario tratarlos directamente durante la aplicaci칩n del modelo. Para esto, se presentan dos t칠cnicas de regresi칩n robusta: RanSac y Huber. RanSac es un muestreo aleatorio de los datos, donde se selecciona una muestra de datos que se consideran no at칤picos y se utilizan para entrenar el modelo. Huber, por otro lado, penaliza los valores at칤picos durante el c치lculo de la p칠rdida y los considera como tales si est치n por encima de un umbral llamado Epsilon. Se ha probado que el valor por defecto de Epsilon, 1.35, funciona eficazmente en el 95% de los casos. En la siguiente clase se implementar치 una regresi칩n robusta para lidiar con datos corruptos en un conjunto de datos sobre la felicidad. Se recomienda explorar diferentes opciones para encontrar la mejor soluci칩n.
--
Ransac: selecciona una muestra aleatoria de los datos asumiendo que esa muestra se encuentra dentro de los valores inliners, con estos datos se entrena el modelo y se compara su comportamiento con respecto a los otros datos. Este procedimiento se repite tantas veces como se indique y al finalizar el algoritmo escoge la combinaci칩n de datos que tenga la mejor cantidad de inliners, donde los valores at칤picos puedan ser discriminados de forma efectiva.
Ejemplo:
Huber Reggresor: no elimina los valores at칤picos sino que los penaliza. Realiza el entrenamiento y si el error absoluto de la perdida alcanza cierto umbral (epsilon) los datos son tratados como at칤picos. El valor por defecto de epsilon es 1.35 ya que se ha demostrado que logra un 95% de eficiencia estad칤stica.
------------robust.py-----------------------
import pandas as pd
import warnings

from sklearn.linear_model import (
    RANSACRegressor, HuberRegressor
)
from sklearn.svm import SVR

from sklearn.model_selection import train_test_split
from sklearn.metrics  import mean_squared_error

if __name__ == "__main__":
    dataset =  pd.read_csv('./data/felicidad_.csv')
    print(dataset.head(5))

    X = dataset.drop(['country', 'score'], axis=1)
    y = dataset[['score']]

    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

    estimadores = {
        'SVR' : SVR(gamma= 'auto', C=1.0, epsilon=0.1),
        'RANSAC' : RANSACRegressor(),
        'HUBER' : HuberRegressor(epsilon=1.35)
    }
    warnings.simplefilter("ignore")
    for name, estimator in estimadores.items():
        estimator.fit(X_train, y_train)
        predictions = estimator.predict(X_test)
        print("="*64)
        print(name)
        print("MSE: ", mean_squared_error(y_test, predictions))

#implementacion_regresion_robusta
---------------------------------------
-----------ensamble-----------------------------
El texto habla sobre los m칠todos de ensamble en el aprendizaje autom치tico, los cuales consisten en combinar diferentes algoritmos o modelos para llegar a una respuesta consensuada y m치s precisa. Se mencionan dos estrategias principales: la primera es el m칠todo de votaci칩n, en el cual se entrena cada modelo en diferentes particiones de los datos y luego se toma la respuesta con mayor cantidad de votos. Se mencionan algunos m칠todos de ensamble reconocidos como el Random Forest. La segunda estrategia es el boosting, en la cual se entrena un clasificador d칠bil varias veces con la retroalimentaci칩n del error de la predicci칩n anterior, para fortalecer el modelo. El texto destaca que la diversidad es importante en estas t칠cnicas y que han sido utilizadas con 칠xito en competiciones de aprendizaje autom치tico.
--bagging----------
import pandas as pd 

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings("ignore")

if __name__ == '__main__':
    dt_heart = pd.read_csv('./data/heart.csv')
    print(dt_heart['target'].describe())

    x = dt_heart.drop(['target'], axis=1)
    y = dt_heart['target']

    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=1)

    knn_class = KNeighborsClassifier().fit(X_train, y_train)
    knn_prediction = knn_class.predict(X_test)
    print('='*64)
    print('SCORE con KNN: ', accuracy_score(knn_prediction, y_test))

    #bag_class = BaggingClassifier(base_estimator=KNeighborsClassifier(), n_estimators=50).fit(X_train, y_train) # base_estimator pide el estimador en el que va a estar basado nuestro metodo || n_estimators nos pide cuantos de estos modelos vamos a utilizar
    #bag_pred = bag_class.predict(X_test)
    #print('='*64)
    #print(accuracy_score(bag_pred, y_test))

    estimators = {
        'LogisticRegression' : LogisticRegression(),
        'SVC' : SVC(),
        'LinearSVC' : LinearSVC(),
        'SGD' : SGDClassifier(loss="hinge", penalty="l2", max_iter=5),
        'KNN' : KNeighborsClassifier(),
        'DecisionTreeClf' : DecisionTreeClassifier(),
        'RandomTreeForest' : RandomForestClassifier(random_state=0)
    }

    for name, estimator in estimators.items():
        bag_class = BaggingClassifier(base_estimator=estimator, n_estimators=50).fit(X_train, y_train)
        bag_predict = bag_class.predict(X_test)
        print('='*64)
        print('SCORE Bagging with {} : {}'.format(name, accuracy_score(bag_predict, y_test)))
------boosting----------

import pandas as pd 

from sklearn.ensemble import GradientBoostingClassifier

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings("ignore")

if __name__ == '__main__':
    dt_heart = pd.read_csv('./data/heart.csv')
    print(dt_heart['target'].describe())

    x = dt_heart.drop(['target'], axis=1)
    y = dt_heart['target']

    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=1)
    
    #boosting = GradientBoostingClassifier(loss='exponential',learning_rate=0.15, n_estimators=100, max_depth=5).fit(X_train, y_train)
    #boosting_pred=boosting.predict(X_test)
    #print('='*64)
    #print(accuracy_score(boosting_pred, y_test))
    
    #obtenemos el mejor resultado junto con el estimador
    estimators = range(1, 300, 1)
    total_accuracy = []
    best_result = {'result' : 0, 'n_estimator': 1}

    for i in estimators:
        boost = GradientBoostingClassifier(n_estimators=i).fit(X_train, y_train)
        boost_pred = boost.predict(X_test)
        new_accuracy = accuracy_score(boost_pred, y_test)
        total_accuracy.append(new_accuracy)
        if new_accuracy > best_result['result']: 
            best_result['result'] = new_accuracy
            best_result['n_estimator'] = i

    print(best_result)
----------------clusterizacion------------------
En este texto se hace referencia a la importancia de abordar los problemas de aprendizaje no supervisado, donde no se tiene informaci칩n previa sobre los datos y se busca descubrir patrones internos. Se menciona que las estrategias de Clustering (agrupamiento de datos) son muy importantes en este tipo de problemas, ya que permiten identificar patrones que no son visibles a simple vista y que pueden ser utilizados posteriormente en modelos de Machine Learning.
Se presentan tres casos de aplicaci칩n de Clustering: cuando no se conocen las etiquetas para una clasificaci칩n de datos, cuando no se sabe nada sobre los datos y se quieren descubrir nuevos patrones, y cuando se quieren identificar valores at칤picos.
Se mencionan dos casos de uso para los algoritmos de Clustering: cuando se sabe cu치ntos grupos se quieren como resultado y cuando no se sabe y se quiere que el algoritmo determine el n칰mero 칩ptimo de grupos. Se recomiendan algunos algoritmos espec칤ficos para cada caso.
Finalmente, se menciona la implementaci칩n de estos problemas en c칩digo y se invita al lector a continuar en la siguiente clase para profundizar en el tema.
--------------K-Means--------------------------------
import pandas as pd
import seaborn as sns
from sklearn.cluster import MiniBatchKMeans

if __name__ == "__main__":

    dataset = pd.read_csv('./data/candy.csv')
    print(dataset.head(10))

    X = dataset.drop('competitorname', axis=1)
    #Selecionamos 4 grupos
    kmeans = MiniBatchKMeans(n_clusters=4, batch_size=8).fit(X)
    print("Total de centros: " , len(kmeans.cluster_centers_))
    print("="*64)
    print(kmeans.predict(X))

    dataset['group'] = kmeans.predict(X)
    sns.pairplot(dataset, hue='group')
    print(dataset)

    #implementacion_k_means
-----variables del dataset candy pareadas------
import pandas as pd
import seaborn as sns
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

if __name__ == "__main__":

    dataset = pd.read_csv('./data/candy.csv')
    #print(dataset.head(10))

    X = dataset.drop('competitorname', axis=1)
    #Selecionamos 4 grupos
    kmeans = MiniBatchKMeans(n_clusters=4, batch_size=8).fit(X)
    print("Total de centros: " , len(kmeans.cluster_centers_))
    print("="*64)
    print(kmeans.predict(X))

    dataset['group'] = kmeans.predict(X)
    print(dataset)
    penguins = sns.load_dataset("penguins")
    sns.pairplot(dataset, hue='group')
    plt.show()
    #implementacion_k_means
----------clustering puede generar los grupos que son imposibles de notar a simple vista.---
import pandas as pd
import seaborn as sns
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

if __name__ == "__main__":

    dataset = pd.read_csv('./data/candy.csv')
    #print(dataset.head(10))

    X = dataset.drop('competitorname', axis=1)
    #Selecionamos 4 grupos
    kmeans = MiniBatchKMeans(n_clusters=4, batch_size=8).fit(X)
    print("Total de centros: " , len(kmeans.cluster_centers_))
    print("="*64)
    print(kmeans.predict(X))

    dataset['group'] = kmeans.predict(X)
    print(dataset)
    sns.pairplot(dataset[['sugarpercent','pricepercent','winpercent','group']], hue = 'group')
    plt.show()
    #implementacion_k_means
---------------Mean Shift-----------
Este texto describe c칩mo utilizar la t칠cnica de clustering, en particular la t칠cnica Mean Shift, para clasificar datos en grupos. Se utiliza Python y la librer칤a Pandas para cargar los datos y hacer el an치lisis. Primero, se carga el archivo de datos y se elimina la columna de nombres. Luego, se define una variable para guardar el modelo de clustering y se utiliza la configuraci칩n por defecto. Se imprime el resultado de los labels y se utiliza la funci칩n Max para determinar el n칰mero de grupos en los datos. Se imprime una l칤nea de separaci칩n y se muestran los centros de cada grupo. Finalmente, se integra el resultado del clustering en la variable tags_edit para utilizarlo en otros modelos o exportarlo.
-- codigo mean Shift-----------
import pandas as pd
from sklearn.cluster import MeanShift

if __name__ == "__main__":

    dataset = pd.read_csv("./data/candy.csv")
    #print(dataset.head(5))

    X = dataset.drop('competitorname', axis=1)

    meanshift = MeanShift().fit(X)
    print(max(meanshift.labels_))
    print("="*64)
    print(meanshift.cluster_centers_)

    dataset['meanshift'] = meanshift.labels_
    print("="*64)
    print(dataset)

    #implementacion_meanshift
------------------------------------
validacion de nuestro modelo Cross Validation
-------
El texto habla sobre la importancia de la validaci칩n de los modelos de Max Hilaire Vim. Se menciona que es crucial ser rigurosos a la hora de evaluar los resultados que se est치n recibiendo, y se compara el proceso de validaci칩n con el de un tester continuo de una aplicaci칩n. Adem치s, se explica que todos los modelos son malos, pero algunos son 칰tiles, y que nunca van a corresponder con la realidad al cien por ciento. Se presentan tres tipos de validaci칩n cruzada: home home, validaci칩n cruzada K-fold y validaci칩n cruzada leave-one-out. Finalmente, se indica cu치ndo es recomendable utilizar cada tipo de validaci칩n cruzada.
---------Cross Validation-----------
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_validate
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import (
    cross_val_score, KFold
)

def train_test_split_kf(data: np.array, target: np.array, train: np.array, test: np.array) -> np.array:
    x_train = data[train]
    x_test = data[test]
    y_train = target[train]
    y_test = target[test]
    return x_train, x_test, y_train, y_test


def evaluate_model(model, metric, x_train, x_test, y_train, y_test):
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    score = metric(y_pred, y_test)
    return score


def main():
    dataset = pd.read_csv("./data/felicidad.csv")

    data = dataset.drop(["country", "score", "rank"], axis=1)
    target = dataset["score"]
    kf = KFold(n_splits=3, shuffle=True, random_state=42)

    models = {"DecisionTreeRegressor": DecisionTreeRegressor()}
    print('---- Easy Implementation ----')
    for name, model in models.items():
        score = cross_val_score(model, data, target, cv=4, scoring='neg_mean_squared_error')
        print("Scores:", score)
        print("Mean score: ", np.abs(np.mean(score)))
        

    print("=" * 64)
    scores = []
    print('---- Full Implementation ----')
    for name, model in models.items():
        print(f"I'm evaluating: {name}")
        for n_fold, (train, test) in enumerate(kf.split(data)):
            print(f"\t-I'm running fold {n_fold + 1}")
            x_train, x_test, y_train, y_test = train_test_split_kf(data.values, target.values, train, test)
            score = evaluate_model(model, mean_squared_error, x_train, x_test, y_train, y_test)
            print("\t\t-score:", score)
            scores.append(score)
        print("="*64)
        print("Scores:", scores)
        print("Mean score: ", np.mean(scores))
       

if __name__ == '__main__':
    main()
------cross. 2--------------
import pandas as pd
import numpy as np

from sklearn.metrics import mean_squared_error

from sklearn.tree import DecisionTreeRegressor

from sklearn.model_selection import (
cross_val_score, KFold
)

dataset = pd.read_csv('./data/felicidad.csv')
X = dataset.drop(['country', 'score'], axis=1)
y = dataset['score']

print(dataset.shape)

model = DecisionTreeRegressor()
score = cross_val_score(model, X,y, cv=3, scoring='neg_mean_squared_error') # con cv podemos controlar el numOfFolds
print(score)
print(np.abs(np.mean(score)))

kf = KFold(n_splits=3, shuffle=True, random_state=42)
mse_values = []

for train, test in kf.split(dataset):
    print(train)
    print(test)

    X_train = X.iloc[train]
    y_train = y.iloc[train]
    X_test = X.iloc[test]
    y_test = y.iloc[test]


model = DecisionTreeRegressor().fit(X_train, y_train)
predict = model.predict(X_test)
mse_values.append(mean_squared_error(y_test, predict))

print("Los tres MSE fueron: ", mse_values)
print("El MSE promedio fue: ", np.mean(mse_values))
-------------Optimizaci칩n param칠trica+++++++++++++++++++
El texto trata sobre la automatizaci칩n de la selecci칩n y optimizaci칩n de modelos de aprendizaje, utilizando el principio de validaci칩n cruzada. En primer lugar, se menciona que encontrar un modelo de aprendizaje que funcione puede llevar a la necesidad de encontrar la optimizaci칩n de cada uno de los par치metros del modelo para obtener el mejor resultado posible. Sin embargo, la tarea de revisar la documentaci칩n y probar todas las combinaciones de par치metros puede ser tediosa y costosa computacionalmente.
Se presentan tres soluciones: la b칰squeda manual, la optimizaci칩n sistem치tica de par치metros y la optimizaci칩n por b칰squeda aleatorizada. La b칰squeda manual implica escoger el modelo y ajustar los n칰meros de los par치metros de forma individual, probando cada combinaci칩n hasta encontrar la adecuada seg칰n la m칠trica. La optimizaci칩n sistem치tica de par치metros utiliza una grilla de par치metros, que se define como una matriz donde se definen los posibles rangos de cada par치metro a trav칠s de un diccionario. Por otro lado, la optimizaci칩n por b칰squeda aleatorizada utiliza la validaci칩n cruzada para probar diferentes combinaciones de par치metros.
Finalmente, se explica c칩mo definir un diccionario para cada par치metro que se quiera probar y c칩mo Saiki Clear puede interpretar el diccionario y probar todas las combinaciones de manera organizada para devolver la mejor combinaci칩n que maximice o minimice la m칠trica de inter칠s.
++++++++++++++++Randomized+++++++++++++++
import pandas as pd

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor

if __name__ == "__main__":

    dataset = pd.read_csv('./data/felicidad.csv')

   # print(dataset)

    X = dataset.drop(['country', 'rank', 'score'], axis=1)
    y = dataset[['score']].squeeze()

    reg = RandomForestRegressor()

    parametros = {
        'n_estimators' : range(4,16),
        'criterion' : ['squared_error', 'absolute_error'],
        'max_depth' : range(2,11)
    }

    rand_est = RandomizedSearchCV(reg, parametros , n_iter=10, cv=3, scoring='neg_mean_absolute_error').fit(X,y)

    print(rand_est.best_estimator_)
    print(rand_est.best_params_)
    print(rand_est.predict(X.loc[[0]]))
+++++++++++++++++++arquitectura de c칩digo+++++++++++++++++++++++++
El texto trata sobre la importancia de organizar los archivos en una estructura de carpetas para facilitar la gesti칩n de un proyecto y su posterior salida a producci칩n. Se mencionan varios archivos necesarios para la ejecuci칩n de un c칩digo modular y extensible, como el archivo "main.py" que define el flujo de ejecuci칩n y otros archivos que se encargan de tareas concretas, como "blog.py" para la carga de elementos y "utils.py" para m칠todos reutilizables. Tambi칠n se menciona la creaci칩n de una clase llamada "utilidades" que contendr치 los m칠todos que se reutilizar치n a lo largo del proceso y otra clase llamada "models" para la parte de machine learning. En resumen, se trata de una gu칤a para crear una estructura de carpetas y archivos bien organizada que facilite la gesti칩n de un proyecto de machine learning.
---------codigo




















